{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp-hw1-word+char.ipynb","provenance":[{"file_id":"1lsWy_yqLdgWk2jA7Hkg3XSUdFq3QX4MH","timestamp":1588278184088},{"file_id":"1OH8BW8RpJB_v-0y4G7kDncLXV685MvI9","timestamp":1588264009213},{"file_id":"1MjanNLhBMU4HP6XMTUmLm0Wui_d6NaEG","timestamp":1588241564252},{"file_id":"1B218TH-FAmRZ93Jveq7Kmd8XjALjciq_","timestamp":1588082512305}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yex9ZCDEfk_2","colab_type":"text"},"source":["# NLPHW1 - Named Entity Recognition\n","Francesco Starna\n","1613660"]},{"cell_type":"markdown","metadata":{"id":"oEMOcBYfggJX","colab_type":"text"},"source":["## Setup\n","Imports."]},{"cell_type":"code","metadata":{"id":"4NnNJoA5P4Nt","colab_type":"code","colab":{}},"source":["import torch\n","from torch.utils.data import IterableDataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","import torch.optim as optim\n","from torch.nn.utils import clip_grad_value_\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","from collections import Counter\n","import csv, pickle\n","\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j7jqXJ4bCfM_","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5bMAnXqpb-i","colab_type":"text"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"PAsUMaxAevgU","colab_type":"text"},"source":["### Vocabulary\n","The class vocabulary it has been thought to create a general vocabulary where each word is lowered and lemmatize. nltk WordNet Lemmatizer is used to get lemma from plurals and verbs."]},{"cell_type":"code","metadata":{"id":"VN_9gx_y3XbR","colab_type":"code","colab":{}},"source":["class Vocab():\n","\n","  def __init__(self, vocab_size, unk_token='<unk>', \n","               path=None, tag_to_index=None, words=None, characters=None):\n","\n","    self.unk_token = unk_token\n","    self.word_vocab_size = vocab_size[0]\n","    self.char_vocab_size = vocab_size[1]\n","\n","    if path is None:\n","      self.__word_vocabulary, self.__char_vocabulary = words, characters\n","      self.tag_to_index = tag_to_index\n","    \n","    else:\n","      self.tag_to_index = {'O':0, 'PER':1, 'ORG':2, 'LOC':3, '<pad>':9, '<test>': 8}\n","      self.__word_vocabulary, self.__char_vocabulary = self.__build_vocabs(path)\n","\n","  def characters(self):\n","    return self.__char_vocabulary\n","\n","  def words(self):\n","    return self.__word_vocabulary\n","\n","  def save(self, path):\n","    with open(path, 'wb') as f:\n","      obj = {'vocab_size' : (self.word_vocab_size, self.char_vocab_size),\n","             'tag_to_index': self.tag_to_index,\n","             'unk_token' : self.unk_token,\n","             'words' : self.words(),\n","             'characters' : self.characters()}\n","      pickle.dump(obj, f)\n","\n","  @staticmethod\n","  def load(path):\n","    with open(path, 'rb') as f:\n","      obj = pickle.load(f)\n","      return Vocab(obj['vocab_size'],\n","                   unk_token=obj['unk_token'], \n","                   tag_to_index=obj['tag_to_index'],  \n","                   words=obj['words'], \n","                   characters=obj['characters'])\n","\n","  def __build_vocabs(self, path):\n","    char_counter = Counter()\n","    word_counter = Counter()\n","    # simplifying vocabulary with lemma (plurals and verbs)\n","    l = WordNetLemmatizer()\n","    with open(path, 'r') as file:\n","      for line in file:\n","        if line[0] == '#':\n","          line = line.split()[1:]\n","          \n","          #populating word vocabulary\n","          for word in line:\n","            word = l.lemmatize(l.lemmatize(word,'v'))\n","            word_counter[word.lower()] += 1\n","              \n","            #populating character vocabulary\n","            for char in word:\n","              char_counter[char] += 1\n","        \n","        else: continue\n","\n","    word_vocabulary = {key:index for index, (key,_) in enumerate(word_counter.most_common(self.word_vocab_size-2))}\n","    word_vocabulary[self.unk_token] = self.word_vocab_size-2\n","    word_vocabulary['<pad>'] = self.word_vocab_size-1\n","    char_vocabulary = {key:index for index, (key,_) in enumerate(char_counter.most_common(self.char_vocab_size-1))}\n","    char_vocabulary[self.unk_token] = self.char_vocab_size-1\n","\n","    return word_vocabulary, char_vocabulary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqZfYcZbhyqC","colab_type":"code","colab":{}},"source":["folder = '/content/drive/My Drive/AIRO/NLP/Homework/homework1/'\n","data_path = 'nlp2020-hw1/data/'\n","vocab_size = (10000,50) #(word_vocab_size, char_vocab_size)\n","\n","vocab = Vocab(vocab_size, path=folder+data_path+'train.tsv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E89aiM2syl3U","colab_type":"text"},"source":["**Saving the Vocabulary**"]},{"cell_type":"code","metadata":{"id":"HHEMYBEcwAx0","colab_type":"code","colab":{}},"source":["vocab.save(folder+'vocab-word+char.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AVZjIb2vepOr","colab_type":"text"},"source":["### Pretrained Embedding\n","The choice of using pretrained embeddings is crucial. The choice was Global Vectors from Standford Wikipedia 2014 + Gigaword 5 with 6B tokens and 50 vector size representation."]},{"cell_type":"code","metadata":{"id":"9SPJVK9zFefU","colab_type":"code","colab":{}},"source":["class GloVe():\n","\n","  def __init__(self, path):\n","\n","    self.vector_dim = 50\n","    self.path = path\n","    self.vectors = self.__load_vectors()\n","\n","  def __contains__(self, item):\n","    return item in self.vectors\n","\n","  def __getitem__(self, item):\n","    return self.vectors[item] if item in self.vectors else None\n","                   \n","  def __load_vectors(self):\n","    vectors_dict = dict()\n","    with open(self.path, 'r') as glove:\n","      for line in glove:\n","        values = line.split()\n","        word = values[0]\n","        vector = [float(i) for i in values[1:]]\n","        vectors_dict[word] = vector\n","    return vectors_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0J6BBHrf-xc","colab_type":"code","colab":{}},"source":["glove = GloVe(folder+\"glove.6B.50d.txt\") "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W1zG5z7o38qS","colab_type":"text"},"source":["### Utility\n","The Encoder class takes care of encoding samples in PyTorch Tensor for feed the neural networks. It also has methods for read text samples and decode output after prediction. For this, it makes use of a Vocab."]},{"cell_type":"code","metadata":{"id":"mfWj0I3X3_7C","colab_type":"code","colab":{}},"source":["class Encoder():\n","\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  l = WordNetLemmatizer()\n","  # output of single layer of LSTM\n","  char_vec_dim = 50\n","\n","  # sampling method for testing\n","  @staticmethod\n","  def read_test(sentence):\n","    sample = list()\n","    for w in sentence.split():      \n","      d = dict()\n","      d['word'] = w\n","      d['lemma'] = Encoder.l.lemmatize(Encoder.l.lemmatize(w,'v'))\n","      d['postag'] = '<test>'\n","      sample.append(d)\n","    return sample\n","\n","  # return the index of the character in the vocabulary\n","  @staticmethod\n","  def __char_2_index(char, v:Vocab):\n","    if char in v.characters(): return v.characters()[char] \n","    else: return v.characters()[v.unk_token]\n","\n","  # returns the index of the word in the vocabulary\n","  @staticmethod\n","  def __word_2_index(word, v:Vocab):\n","    if word in v.words(): return v.words()[word]\n","    else: return v.words()[v.unk_token]\n","\n","  @staticmethod\n","  def load_word_embeddings(v:Vocab, g:GloVe):\n","    embeddings = torch.randn(len(v.words()), g.vector_dim)\n","    for i, w in enumerate(v.words()):\n","      if w in g: embeddings[i] = torch.FloatTensor(g[w])\n","    return embeddings\n","\n","  @staticmethod\n","  def load_char_embeddings(v:Vocab):\n","    embeddings = torch.randn(len(v.characters()), Encoder.char_vec_dim)\n","    # one hot encoding\n","    for i, w in enumerate(v.characters()):\n","      vector = [0]*Encoder.char_vec_dim\n","      vector[i] = 1\n","      embeddings[i] = torch.FloatTensor(vector)\n","    return embeddings\n","\n","  \"\"\"\n","    input:  list of dictionaries each per word in sentence\n","    output: vocabulary in which\n","      encoded_words  -> Tensor of [indexes of words in a sample] + <PAD>\n","      encoded_chars  -> lists of [Tensors of [indexes of chars of word] in a sample] \n","      encoded_labels -> Tensor of [index of postag] + <PAD>\n","  \"\"\"\n","  @staticmethod\n","  def encode(sample, v:Vocab):\n","\n","    encoded_words = torch.LongTensor([Encoder.__word_2_index(w['lemma'].lower(), v) for w in sample]).to(Encoder.device)\n","  \n","    encoded_tags = torch.LongTensor([v.tag_to_index[w['postag']] for w in sample]).to(Encoder.device)\n","    \n","    encoded_chars = [torch.tensor([Encoder.__char_2_index(ch, v) for ch in w['word']]).to(Encoder.device) for w in sample]\n","\n","    return {'words' : encoded_words, 'chars' : encoded_chars, 'tags' : encoded_tags}\n","\n","  # Decoding the prediction\n","  @staticmethod\n","  def decode(indices, v:Vocab):\n","    # reverse tag_to_index mapping in order to check output indices\n","    index_to_tag = {v:k for k,v in v.tag_to_index.items()}\n","    indices = indices.squeeze().tolist()\n","    if type(indices) is list: return [index_to_tag[tag] for tag in indices]\n","    else: return [index_to_tag[indices]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75u3S4tgEjuU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1588367245524,"user_tz":-120,"elapsed":8671,"user":{"displayName":"LORENZO STARNA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKR5v284cgaR27wzEs0dN-HeCZMmkiMSiIJPJBYQ=s64","userId":"14898881167610599499"}},"outputId":"3fd4beb7-67a4-4e94-ca04-c3af70e5cdfd"},"source":["sample = [{'word':'home','lemma':'home','postag':'O'},{'word':'bellissima','lemma':'bellissima','postag':'ORG'}, {'word':'Francesco','lemma':'Francesco','postag':'<test>'}]\n","print('[DEVICE]', Encoder.device)\n","encoded_sample = Encoder.encode(sample, vocab)\n","print(encoded_sample['words'].shape, encoded_sample['words'])\n","print(encoded_sample['chars'])\n","print(encoded_sample['tags'].shape, encoded_sample['tags'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[DEVICE] cpu\n","torch.Size([3]) tensor([ 152, 9998, 8695])\n","[tensor([ 7,  3, 13,  0]), tensor([15,  0,  9,  9,  5,  8,  8,  5, 13,  2]), tensor([42,  6,  2,  4, 10,  0,  8, 10,  3])]\n","torch.Size([3]) tensor([0, 2, 8])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qmFq9KNJrkwc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1588368486337,"user_tz":-120,"elapsed":690,"user":{"displayName":"LORENZO STARNA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKR5v284cgaR27wzEs0dN-HeCZMmkiMSiIJPJBYQ=s64","userId":"14898881167610599499"}},"outputId":"c55bee47-eb27-45bd-ba7c-053e55f28cb7"},"source":["# character representation of word with BiLSTM\n","def char_bilstm(word, v:Vocab):\n","  chars = torch.rand(len(word), 50)\n","  bilstm = nn.LSTM(50, 25, bidirectional=True, batch_first=True)\n","  _, (hidden, _) = bilstm(chars.unsqueeze(0))\n","  return torch.cat((hidden[0], hidden[1]),1)\n","\n","char_bilstm('ciao', vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0643, -0.1940, -0.1719, -0.1472, -0.0175, -0.1040,  0.1489, -0.1840,\n","          0.1471,  0.0731,  0.1022,  0.2541, -0.1660,  0.0427, -0.2378,  0.0442,\n","         -0.0412, -0.0569, -0.1550, -0.1837,  0.2991, -0.0095,  0.1728, -0.0198,\n","         -0.0535,  0.2710,  0.1536, -0.0456, -0.1888, -0.2756, -0.2404,  0.0736,\n","          0.2944, -0.1523,  0.3258, -0.0996,  0.2182, -0.0096, -0.0377,  0.0558,\n","         -0.2185,  0.1225,  0.0824, -0.0670,  0.0667, -0.2817,  0.0253,  0.1209,\n","          0.1276, -0.1604]], grad_fn=<CatBackward>)"]},"metadata":{"tags":[]},"execution_count":157}]},{"cell_type":"markdown","metadata":{"id":"rFrmCUDFey_c","colab_type":"text"},"source":["### Dataset\n","The class organizes input text and samples it using a dictionary for each word \n","\n","```\n","{'word':'word', 'lemma':'lemma of word', 'postag':'tag of word'}\n","```\n","It is subclass of IterabaleDataset, where the __ iter __ method returns inputs and tags encoding the dictionary representation of each sentence."]},{"cell_type":"code","metadata":{"id":"1nkWU_UCpd6V","colab_type":"code","colab":{}},"source":["class MyDataset(IterableDataset):\n","\n","  def __init__(self, path, vocab:Vocab):\n","\n","    # max lengths of sample for padding\n","    self.max_s_len = 0\n","    self.vocab = vocab\n","    self.path = path\n","    self.__data = self.__build_dataset()\n","\n","  def __len__(self):\n","    return len(self.__data)\n","\n","  def __getitem__(self, index):\n","    return self.__data[index]\n","\n","  \"\"\" \n","    build list of lists of dictionaries\n","    sample --> [{word-1: _ , lemma-1: _ , postag-1: _ }...{word-n: _ , lemma-n: _ , postag-n: _ }]\n","  \"\"\"\n","  def __build_dataset(self):\n","    data = list()\n","    with open(self.path, 'r') as file:\n","\n","      for line in file:\n","        line = line.split()\n","        if len(line) == 0: continue\n","        elif line[0] == '#':\n","          count = len(line[1:])\n","          sample = list()\n","\n","          for i in range(count):\n","            line = next(file).split()\n","            d = dict()\n","            d['word'] = line[1]\n","            d['lemma'] = Encoder.l.lemmatize(Encoder.l.lemmatize(line[1].lower(),'v'))\n","            d['postag'] = line[2]\n","            sample.append(d)\n","\n","          if int(line[0]) > self.max_s_len: self.max_s_len = int(line[0])+1\n","          data.append(sample)\n","\n","    return data\n","\n","  # ecoded dataset \n","  def __iter__(self):\n","    for i in range(len(self.__data)):\n","      yield Encoder.encode(self.__data[i], self.vocab)\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"myx8c6fArgTC","colab_type":"code","colab":{}},"source":["train_set = MyDataset(folder+data_path+'train.tsv', vocab)\n","dev_set = MyDataset(folder+data_path+'dev.tsv', vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vGDNkY_ivdFk","colab_type":"text"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"FFZ6awu_elnv","colab_type":"text"},"source":["### Building\n","The Neural Network model is based on BiLSTM and make use of a dropout layer to prevent overfitting and finally returns a logits prediction where each one represent the probability of each class.\n"]},{"cell_type":"code","metadata":{"id":"ovLiyvQZvhFb","colab_type":"code","colab":{}},"source":["class MyModel(nn.Module):\n","\n","  def __init__(self, hparams):\n","    super(MyModel, self).__init__()\n","\n","    # Word embedding\n","    self.word_embedding = nn.Embedding(hparams.word_vocab_size, hparams.word_embedding_dim)\n","    self.word_embedding.weight.data.copy_(hparams.word_embeddings)\n","\n","    # Char embedding\n","    self.char_embedding = nn.Embedding(hparams.char_vocab_size, hparams.char_embedding_dim)\n","    self.char_embedding.weight.data.copy_(hparams.char_embeddings)\n","\n","    # Bidirectional Char LSTM\n","    self.char_lstm = nn.LSTM(hparams.char_embedding_dim, hparams.char_hidden_size, \n","                          batch_first=True,\n","                          bidirectional=True)\n","\n","    # Bidirectional LSTM \n","    self.lstm = nn.LSTM(hparams.embedding_dim, hparams.hidden_dim,\n","                        batch_first=True,\n","                        bidirectional=True)\n","    \n","    # dropout embedding\n","    self.dropout = nn.Dropout(hparams.dropout)\n","    \n","    # linear classifier\n","    self.classifier = nn.Linear(hparams.hidden_dim*2, hparams.num_tags)\n","\n","  def forward(self, w, c):\n","    word_embedding = self.word_embedding(w)\n","    emb = []\n","\n","    for i in range(len(w)):\n","      chars = torch.cat([self.char_embedding(ch).unsqueeze(0) for ch in c[i]])\n","      self.init_hidden(self.char_lstm)\n","      _, (hidden, _) = self.char_lstm(chars.unsqueeze(0))\n","      emb.append(torch.cat((hidden[0], hidden[1]),1).squeeze(0).tolist())\n","      \n","    char_embedding = torch.FloatTensor(emb).to(Encoder.device)\n","    embedding = torch.cat((word_embedding, char_embedding),1)\n","    x = self.dropout(embedding)\n","    o, (_, _) = self.lstm(x.unsqueeze(0))\n","    output = self.classifier(o)\n","    return output\n","\n","  def init_hidden(self, lstm:nn.LSTM):\n","    for name, param in lstm.named_parameters():\n","      if 'bias' in name:\n","        nn.init.constant_(param, 0.0)\n","      elif 'weight' in name:\n","        nn.init.xavier_normal_(param)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSWBNfRLeZ2A","colab_type":"text"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"Z_vmDqlQedfm","colab_type":"code","colab":{}},"source":["class HParams():\n","\n","  def __init__(self, v:Vocab, g:GloVe):\n","\n","    # word\n","    self.word_vocab_size = v.word_vocab_size\n","    self.word_embedding_dim = g.vector_dim\n","    self.word_embeddings = Encoder.load_word_embeddings(v, g)\n","\n","    # char\n","    self.char_vocab_size = v.char_vocab_size\n","    self.char_embedding_dim = Encoder.char_vec_dim\n","    self.char_hidden_size = int(self.char_embedding_dim/2)\n","    self.char_embeddings = Encoder.load_char_embeddings(v)\n","\n","    # parameters\n","    self.embedding_dim = 100\n","    self.num_layers = 1\n","    self.hidden_dim = 128\n","    self.dropout = 0.2\n","    self.num_tags = len(v.tag_to_index)-2\n","    self.epochs = 5\n","\n","params = HParams(vocab, glove)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKxdiKz2hU7g","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"YBH3i0FPhXhA","colab_type":"code","colab":{}},"source":["class Trainer():\n","\n","  def __init__(self, model,loss_function, optimizer):\n","    self.model = model\n","    self.loss_function = loss_function\n","    self.optimizer = optimizer\n","\n","  def train(self, train_dataset, valid_dataset, epochs=1):\n","    \n","    train_loss = 0.0\n","    \n","    for epoch in range(epochs):\n","      print(' Epoch {:03d}'.format(epoch + 1))\n","\n","      # train mode\n","      self.model.train()\n","\n","      epoch_loss = 0.0\n","\n","      for step, sample in enumerate(train_dataset):\n","        words = sample['words']\n","        chars = sample['chars']\n","        tags = sample['tags']\n","        \n","        if step % 10 == 0: print('\\r\\t[training = {:0.2f} %]'.format(step*100/len(train_dataset)), end=\"\")\n","        self.optimizer.zero_grad()\n","        predictions = self.model(words, chars)\n","\n","        predictions = predictions.view(-1, predictions.shape[-1])\n","        tags = tags.view(-1)\n","  \n","        sample_loss = self.loss_function(predictions, tags)\n","        sample_loss.backward()\n","\n","        #gradient clipping\n","        #clip_grad_value_(self.model.parameters(), 5)\n","\n","        self.optimizer.step()\n","\n","        epoch_loss += sample_loss.tolist()\n","      \n","      avg_epoch_loss = epoch_loss/len(train_dataset)\n","      train_loss += avg_epoch_loss\n","      print('\\n\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch+1, avg_epoch_loss))\n","\n","      valid_loss = self.evaluate(valid_dataset)\n","      print('\\t[E: {:2d}] valid loss = {:0.4f}'.format(epoch+1, valid_loss))\n","\n","    avg_epoch_loss = train_loss / epochs\n","    return avg_epoch_loss\n","\n","  def evaluate(self, valid_dataset):\n","    valid_loss = 0.0\n","    \n","    # evaluation mode\n","    self.model.eval()\n","\n","    with torch.no_grad():\n","      for sample in valid_dataset:\n","\n","        words = sample['words']\n","        chars = sample['chars']\n","        tags = sample['tags']\n","        \n","        predictions = self.model(words, chars)\n","        predictions = predictions.view(-1, predictions.shape[-1])\n","        tags = tags.view(-1)\n","        sample_loss = self.loss_function(predictions, tags)\n","\n","        valid_loss += sample_loss.tolist()\n","\n","    return valid_loss / len(valid_dataset)\n","\n","  def predict(self, x):\n","    # evaluation mode\n","    self.model.eval()\n","    with torch.no_grad():\n","      words = x['words']\n","      chars = x['chars']\n","      logits = self.model(words, chars)\n","      predictions = torch.argmax(logits, -1)\n","      return logits, predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ovC0OOX6lax","colab_type":"code","colab":{}},"source":["import time\n","\n","start = time.time()\n","for step, sample in enumerate(train_set):\n","  if step == 10000: break\n","print(time.time()-start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBnSWcmn3u6F","colab_type":"code","colab":{}},"source":["postagger = MyModel(params).cuda() if Encoder.device == 'cuda' else MyModel(params)\n","\n","trainer = Trainer(\n","    model = postagger,\n","    loss_function = nn.CrossEntropyLoss(),\n","    #optimizer = optim.SGD(postagger.parameters(), lr=0.01)\n","    optimizer = optim.Adam(postagger.parameters())\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBzAD-TyMiNb","colab_type":"code","colab":{}},"source":["trainer.train(train_set, dev_set, params.epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8ULSiCRhYf0","colab_type":"text"},"source":["## Evaluation\n","Accuracy it is not a good estimator. In fact, even with high accuray, we could have low precision (due to unbalanced sets)."]},{"cell_type":"code","metadata":{"id":"WDfLei0_hYqo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1588403409766,"user_tz":-120,"elapsed":283901,"user":{"displayName":"LORENZO STARNA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKR5v284cgaR27wzEs0dN-HeCZMmkiMSiIJPJBYQ=s64","userId":"14898881167610599499"}},"outputId":"22317b23-31b4-48b3-999a-5243c98d28bd"},"source":["test_set = MyDataset(folder+data_path+'test.tsv', vocab)\n","test_set_loss = trainer.evaluate(test_set)\n","print(\"test set loss: {}\".format(test_set_loss))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test set loss: 0.09673682880572643\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TyiG44EobFds","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1588403692780,"user_tz":-120,"elapsed":283011,"user":{"displayName":"LORENZO STARNA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKR5v284cgaR27wzEs0dN-HeCZMmkiMSiIJPJBYQ=s64","userId":"14898881167610599499"}},"outputId":"870a1571-6c0e-42b7-820b-bfe6971f1892"},"source":["count, tot_samples = 0,0\n","tags = dict.fromkeys(vocab.tag_to_index, 0)\n","del tags['<test>']\n","del tags['<pad>']\n","matrix = dict()\n","for tag in tags:\n","  matrix[tag] = tags.copy()\n","precision = dict()\n","recall = dict()\n","\n","for sample in test_set:\n","\n","  test_y = sample['tags']\n","  \n","  _, predictions = trainer.predict(sample)\n","  \n","  x_tags = Encoder.decode(predictions,vocab)\n","  y_tags = Encoder.decode(test_y,vocab)\n","\n","  for x, y in zip(x_tags, y_tags):\n","    tot_samples += 1\n","    if x == y: \n","      count += 1\n","    matrix[x][y] +=1  \n","\n","for key in matrix:\n","  precision[key] = matrix[key][key]/sum([matrix[key][k] for k in matrix])\n","  recall[key] = matrix[key][key]/sum([matrix[k][key] for k in matrix])\n","\n","print('[ACCURACY] =',count/tot_samples)\n","print('[PRECISION]','[RECALL]','[F1 SCORE]')\n","for tag in precision:\n","  f1_score = 2*precision[tag]*recall[tag]/(precision[tag]+recall[tag])\n","  print('{}\\t{:0.2f}\\t{:0.2f}\\t{:0.2f}'.format(tag, precision[tag], recall[tag], f1_score))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ACCURACY] = 0.9706953341389599\n","[PRECISION] [RECALL] [F1 SCORE]\n","O\t0.98\t0.99\t0.99\n","PER\t0.91\t0.88\t0.90\n","ORG\t0.81\t0.64\t0.72\n","LOC\t0.84\t0.82\t0.83\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0citQ3iV9E9R","colab_type":"text"},"source":["Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"AvLC5VrV5hpx","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib as plt\n","import seaborn as sns; sns.set()\n","\n","confusion = dict()\n","for key in matrix:\n","  factor=1.0/sum(matrix[key].values())\n","  n = {k:v*factor for (k,v) in matrix[key].items()}\n","  confusion[key] = n\n","\n","data = [np.array(list(d.values())) for d in confusion.values()]\n","ax = sns.heatmap(data, vmin=0, vmax=0.2, square=True, cmap='coolwarm', annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDKiJRHI2jZ0","colab_type":"text"},"source":["## Prediction\n","A prediction method for testing every single sentence"]},{"cell_type":"code","metadata":{"id":"FXWLOXjQ2ghP","colab_type":"code","colab":{}},"source":["sentence = 'Francesco went with Lorenzo to the airport in Rome'\n","sample = Encoder.read_test(sentence)\n","encoded_sample = Encoder.encode(sample, vocab)\n","_, predictions = trainer.predict(encoded_sample)\n","Encoder.decode(predictions, vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qu3cHL644f5","colab_type":"text"},"source":["**Saving the Model**"]},{"cell_type":"code","metadata":{"id":"FmQIdyNsYGR1","colab_type":"code","colab":{}},"source":["torch.save(postagger.state_dict(), folder+'postagger-word+char.txt')"],"execution_count":null,"outputs":[]}]}